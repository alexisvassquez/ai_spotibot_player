# AudioScript IR (Intermediate Representation)

AudioScript IR is the **engine-facing, deterministic representation** of the AudioScript program.

It is generated by the AudioScript compiler, AudioMIX analyzers, AI system, and export tools.
It is the primary format consumed by the AudioMIX runtime for playback and automation.

AudioScript IR is **not intended to be written by hand** during live performance.

---

## Why AudioScript IR Exists

AudioScript is intentionally split into two layers:

- **AudioScript Live**
  - Human-authored
  - Used for live coding, creative control, and reactive behavior
  - Expressive and ergonomic

- **AudioScript IR (this layer)**
  - Machine-generated
  - Deterministic and schedulable
  - Stable over time
  - Engine-executable

This separation allows AudioScript Live to evolve freely without breaking
existing projects, while the engine depends on a minimal, well-defined contract.

The AudioMIX engine primarily executes **AudioScript IR**, not Live syntax.

---

## Design Goals

AudioScript IR is designed to be:

- **Deterministic**
  Given the same IR file, the engine will always schedule and execute the same
  events in the same order.

- **Explicit**
  All behavior is anchored to absolute time and concrete commands.

- **Engine-friendly**
  IR avoids control flow, reactive hooks, and dynamic evaluation.

- **Tool-friendly**
  IR can be generated by:
  - Audio analysis pipelines
  - AI-assisted composition systems
  - UI-based arrangers
  - Offline compilers

---

## High-Level Structure

An AudioScript IR file describes a single track with tempo, sections, and
time-scheduled commands.

As per the example file located in this directory `cvltiv8r_clean.as`:

```python
track "cvltiv8r_clean" {
    bpm 128

    section "intro" from 0.000 to 8.000 {
        at 0.000 {
            led.ambient(front_strip, intensity: 0.30);
        }
    }
}
```

`cvltiv8r_clean.as` is a snippet of a generated `.as` file using the AudioScript compiler from a demo track.
The AudioScript compiler can be viewed in directory `audio/ai/compiler/` of this project.

- **Track:**
  - Defines the playable unit
  - Typically derived from a source asset (e.g., WAV stem, mp3)
  - Contains tempo and sections

- **BPM:**
  - Declares the tempo for the track
  - Used for UI, visualization, and future beat-based features
  - IR v1 scheduling is expressed in seconds

- **Section:**
  - A labeled time range
  - Primarily for organization and navigation
  - Not required for deterministic playback

- **At Blocks:**
  - Schedule one or more commands at an absolute timestamp (in seconds)
  - Commands inside an `at` block execute in source order

- **Commands:**
  - Qualified subsystem calls (e.g., `eq.preset`, `led.scene`)
  - Arguments may be positional or named
  - Commands are dispatched through the *Creative Operating Layer (COL)*, the AudioMIX DSP system

- **Time Semantics:**
  - All timestamps are *absolute seconds* (floating point)
  - Multiple `at` blocks may share the same timestamp
  - Execution order is stable and follows file order
  - *Note:* the full `cvltiv8r_clean.as` file (located on my local device) is very long, which is why I provided a snippet

---

### Execution Model

At runtime, the engine:
1. Parses the IR into an in-memory schedule
2. Starts a transport clock
3. Dispatches commands when the clock reaches their timestamp
4. Forwards commands to subsystem handlers (DSP, lighting, visuals, haptics, etc.)

IR execution occurs on a *control thread*, not on the audio callback.
All real-time safety constraints are enforced inside the subsystem modules.

---

## Relationship to AudioScript Live

AudioScript Live programs *compile into AudioScript IR.*

Live features such as:
  - `on mood(...) {...}`
  - FX chaining
  - Higher-level abstractions
are lowered into explicit IR commands before exection

---

### Versioning

AudioScript IR is intended to be versioned and backward-compatible.

IR v1 is intentionally minimal and focuses on:
  - Audio parameter control
  - EQ and mixer automation
  - Lighting and performance cues

Future versions to be introduced:
  - Beat-based scheduling
  - Additional subsystems (visuals, haptics)
  - Optional headers or metadata blocks
